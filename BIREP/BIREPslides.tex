\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usetheme{Copenhagen}
\usepackage{multirow}
\usepackage{braids}
\usepackage[]{graphicx}
\usepackage{rotating}
\usepackage{pgf,tikz}
\usepackage{pgfplots}
\usepackage{tikz-cd}
\usepackage{xcolor}

\usetikzlibrary{arrows}
\usetikzlibrary{cd}
\usetikzlibrary{babel}
\pgfplotsset{compat=1.13}


\theoremstyle{definition}

\newtheorem{teorema}{Theorem}
\newtheorem{defi}{Definition}
\newtheorem{prop}[teorema]{Proposition}

\newcommand{\R}{\mathbb{R}}


\DeclareMathOperator{\argmax}{argmax}


\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    %\insertframenumber/\inserttotalframenumber
}
\setbeamercolor{footline}{fg=black}
\setbeamerfont{footline}{series=\bfseries}

\newcommand{\highlight}[1]{%
	\colorbox{red!50}{$\displaystyle#1$}}

\makeatletter
\newcommand*{\encircled}[1]{\relax\ifmmode\mathpalette\@encircled@math{#1}\else\@encircled{#1}\fi}
\newcommand*{\@encircled@math}[2]{\@encircled{$\m@th#1#2$}}
\newcommand*{\@encircled}[1]{%
	\tikz[baseline,anchor=base]{\node[draw,circle,outer sep=0pt,inner sep=.2ex] {#1};}}
\makeatother

\expandafter\def\expandafter\insertshorttitle\expandafter{%
  \insertshorttitle\hfill%
  \insertframenumber\,/\,\inserttotalframenumber}

%-----------------------------------------------------------

\title{Clustering with ToMATo}
\author{Javier Aguilar Mart\'in}
\institute{BIREP 2023}
\date{}
 
\begin{document}
\frame{\titlepage}




\setbeamercovered{highly dynamic}

\newcounter{saveenumi}
\newcommand{\seti}{\setcounter{saveenumi}{\value{enumi}}}
\newcommand{\conti}{\setcounter{enumi}{\value{saveenumi}}}

\makeatletter
\newcommand{\xRightarrow}[2][]{\ext@arrow 0359\Rightarrowfill@{#1}{#2}}
\makeatother

\resetcounteronoverlays{saveenumi}



\section{Clustering}

\begin{frame}
\frametitle{Clustering}
\begin{itemize}
\item \textbf{Clustering} is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). %typically by distance %it is an ill posed problem in general

\begin{figure}
\includegraphics[scale=0.5]{cluster}
\end{figure}
\end{itemize}
\end{frame}

\subsection{Classical methods}
\begin{frame}
\frametitle{Classical methods}
\begin{itemize}
\frametitle{}
\item \textbf{Density clustering}: assume that data points are drawn from
some unknown density function $f$. Thresholding the density at some fixed level $\alpha$, then treating the connected components
of the superlevel-set $F^{\alpha} = f^{-1}([\alpha,+\infty))$ as clusters and the rest of the data as noise. %density must be estimated, we will estudy how the superlevel-sets evolve with alpha
\item \textbf{Mode-seeking}: detecting local peaks of $f$ in order to use them as cluster centers and partition the data according to their
\emph{basins of attraction}. %essentialy the points that are attracted follwing the gradient to the local peak 
\end{itemize}
\end{frame}

\begin{frame}
\begin{figure}
\includegraphics[scale=0.5]{basin}
\caption{Basin of attraction.}
\end{figure}
\end{frame}

\subsection{Topological persistence}
\begin{frame}
\begin{itemize}
\item \textbf{Topological persistence}: Study the topology of the superlevel-sets $F^{\alpha} = f^{-1}([\alpha,+\infty))$ as $\alpha$ ranges from $+\infty$ to $-\infty$ to estimate the \emph{prominence} of the density peaks. %alpha goes backwards  %prominence is the lifespan of connected components
\begin{figure}
\includegraphics[scale=0.6]{density}
\caption{(a) original density and its superlevel-sets; (b) a
piecewise-linear approximation $\tilde{f}$ of $f$; (c) superimposition of the PDs of $f$ (red) and $\tilde{f}$ (blue).} %components are created and die %prominent peaks are matched %the peaks tells us that the clusters are the basins of attraction %the density just means that there are more points concentrated near the pics
\end{figure}
\end{itemize}
\end{frame}

\begin{frame}
%to make more clear that the density is not the data
\begin{figure}
\includegraphics[scale=0.7]{ex}
\caption{(a) estimation of the underlying density function $f$ at the data points;
(b) initial clusters; (c) approximate PD showing 2 points far off the diagonal
corresponding to the 2 prominent peaks of $f$; (d) final result obtained after merging the clusters of
non-prominent peaks.}
\end{figure}
\end{frame}

\section{ToMATo}
\subsection{Continuous setting}
\begin{frame}[fragile]
\frametitle{Continuous setting}
\begin{itemize}
\item<1-> Let $X$ be a $n$-dimensional Riemannian manifold without boundary and $f:X\to\R$ a bounded above and proper Morse function with finitely many critical points.
\item<2-> Let $m$ be a critical point of $f$. The \textbf{ascending region} $A(m)$ is the set of points of $X$ that eventually reach $m$ moving along the gradient flow of $f$. For all $x\in A(m)$ we call $m=r(x)$ the \textbf{root} of $x$.

\item<3-> For $\alpha\in\R$ let $F^\alpha=f^{-1}([\alpha,+\infty))$ be a superlevel-set. The family $\{F^\alpha\}_{\alpha\in\R}$ is called the \textbf{superlevel-set filtration} of $f$. Denote $C(x,\alpha)\subseteq F^\alpha$ the path-connected component of $F^\alpha$ containing $x$. %consider alpha varying from +infty to -infty
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\item<1-> Let $m_p$ be a peak of $f$ generating a component at time $p_x=f(m_p)$. Let $p_y\leq p_x$ be the time at which this component merges another component generated by a peak $m_q$. %The reason for the notation is the coordinates in persistence diagram
\item<2->We call $m_q=r(m_p)$ the \textbf{root} of $m_p$ and we say that $m_p$ is \textbf{$(p_x-p_y)$-prominent} or that the \textbf{prominence} of $m_p$ is $p_x-p_y\geq 0$.
\item<3-> Let $\tau\geq 0$. Iterate the root map $m_q\to r(m_q)$ until a peak of prominence $\tau$ is reached. Call this iterated root $r^*_\tau$.
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{Basins of attraction}
\begin{itemize}
\item<1-> The union of the ascending regions of the peaks mapped to $m_p$ through $r^*_\tau$ is referred to as the \textbf{basin of attraction}.
\item<2-> $\forall m_p$ such that $p_x - py \geq\tau$
\[B_\tau(m_p)=\bigcup_{r^*_\tau(m_q)=m_p} A(m_q)\]
\item<3->[]The basins of attraction form a partition of the union of all ascending
regions. These basins are our target clusters.
\end{itemize}

\end{frame}

\subsection{Discrete setting}
\begin{frame}
\frametitle{Discrete setting}
\begin{itemize}
\item<1-> \textbf{Input}: unweighted simple graph $G$, whose
vertex set represents the data points and whose edges connect the points that are similar. %that are close together, user-defined
\begin{itemize}
\item<1-> Each vertex $i$ of $G$ must be assigned a value $\tilde{f}(i)\geq 0$ corresponding to the estimated density at that point.
\end{itemize}
\item<2-> Merging parameter $\tau\geq 0$.
\item<3-> \textbf{Output}: Subset of clusters $e$ whose root $r(e)$ are the peaks of $\tilde{f}$ with prominence at least $\tau$ and such that $\tilde{f}(r(e))\geq \tau$. %last bit to discard noise from sparse data that never gets merged
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Algorithm}
\textbf{Mode-seeking}

\begin{itemize}
\item<2-> Iterate over the vertices of $G$ sorted by decreasing $\tilde{f}$-values.
\item<3-> Connect vertex $i$ to its neighbour with highest $\tilde{f}$-value if that value is higher than $\tilde{f}(i)$. Otherwise, $i$ is declared a peak of $\tilde{f}$. %it simulates the effect of the gradient of the underlying density function
\end{itemize}  
\begin{itemize}
\item<4->[] The resulting collection of
pseudo-gradient edges forms a spanning forest of the graph, and each tree in this
forest is analogue to the ascending region of a peak of in the continuous setting.
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{Algorithm}
\textbf{Merging}:
\begin{itemize}
\item<2-> Let $U$ be union-find data structure where each entry corresponds to a union of trees of the spanning forest.
\item<3-> We call \emph{root} of an entry $e$, or $r(e)$, the vertex contained in $e$ whose $\tilde{f}$-value is highest.
\item<4-> Iterate again over the vertices of $G$ in the same order: 
\begin{itemize}
\item<5-> If $i$ is a local peak of $\tilde{f}$, i.e. $i$ is the root of some tree $T$, create a new entry $e$ in $U$ containing $T$ and set $r(e)=i$.%bein a local peak of tilde f in G is the same as beeing the root of some tree
\item<6-> Otherwise, let $e_i$ the entry of $U$ to which $i$ belongs. Compute the set of entries $E\subseteq U$ that contain the neighbours of $i$. For each $e\in E$, if $e\neq e_i$ and $\min\{\tilde{f}(e),\tilde{f}(e_i)\}<\tilde{f}(i)+\tau$, then merge $e$ and $e_i$ into $e\cup e_i$ in $U$ and set $r(e\cup e_i)=\argmax_{\{r(e),r(e_i)\}}\tilde{f}$. %this determines the prominence, because if a cluster is merged before reaching a vertex that is close in height to the root, the prominence is precisely that height difference because components are always born at the peak and die when merged
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Algorithm}
\begin{itemize}
\item<1-> Upon termination, the merged clusters stored in the entries of the union-find data
structure $U$ form a partition of the vertex set of $G$, and their roots are the peaks of $\tilde{f}$
of prominence at least $\tau$ within the graph. 

\item<2-> The output of ToMATo is then the subset of this collection of clusters that is stored in those entries $e$ such that $\tilde{f}(r(e))\geq \tau$. 

\item<3-> The rest of the data points is treated as
background noise and discarded from the data set.

\item<4-> ToMATo can also output lifespans of all the entries: an entry is born when it is created with a single tree and dies when it is merged into another entry with higher root.
\end{itemize}
\end{frame}

\subsection{Complexity}
\begin{frame}
\frametitle{Time complexity}
Let $n$ be the number of vertices of $G$ and $m$ the number of edges.
\begin{itemize}
\item<2-> Mode-seeking and merging can be run simultaneously: for each vertex $i$ in $G$, compute the gradient at $i$,
then perform merges in the union-find data structure $U$ as these involve only previously visited vertices.
\item<3-> Mode-seeking phase takes a linear time in $n$ once the vertices have been sorted.
\item<4-> Merging makes $O(n)$ union and $O(m)$ find queries to $U$. %union-find data structure
\begin{itemize}
\item <5->This is known to take $O(n \log n+m\alpha(n))$ where $\alpha$ is the inverse Ackerman function. %worst case scenario under an appropriate representation of U, gradients and roots stored in separate containers
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Space complexity}
\begin{itemize}
\item<1-> The graph $G$ does not have to be stored entirely in main memory: only the neighbourhood of vertex $i$ is involved at the
$i$-th iteration of the clustering procedure. 

\item<2-> The main memory usage is thus reduced to $O(n)$.

\item<3-> The total space complexity remains $O(n + m)$ as the graph needs to be stored somewhere.
\end{itemize}
\end{frame}

\section{Parameter selection}
\begin{frame}
\frametitle{Parameter selection}
\textbf{Neighbourhood graph $G$}
\begin{itemize}
\item<1-> $\delta$-Rips graph connects two data points whenever they lie within distance $\delta$ of each other.
\begin{itemize}
\item<2-> Sound theoretical framework (to be seen later).
\item<3-> The choice of a $\delta$ corresponds to the choice of a scale at which to inspect the data.
\item<4-> Run the algorithm for several values of $\delta$. %small values produce to many small peaks, too large values misses the structure
\end{itemize}
\item<5-> $k$-nearest neighbour graph works in practice but lacks theoretical background. Need to choose $k$.
\item<6-> Delaunay graph and variants. Parameter free but usually needs some tweaking.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Parameter selection}
\textbf{Density estimator $\tilde{f}$}
\begin{itemize}
\item<1-> The algorithm is agnostic to the choice of estimator. 
\item<2-> Popular choice: truncated Gaussian kernel estimator. %the other one is distance to a measure, but it is not popular, would have to check the reference
%use truncated normal distribution as kernel, and then f(x) = K(x-x_i/h)/n
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Parameter selection}
\textbf{Merging parameter $\tau$}
\begin{itemize}
\item<1-> The choice of $\tau$ determines which peaks of $\tilde{f}$ are considered significant. %because peaks of less prominence are merged into more prominent peaks
\item<2-> Run ToMATo twice: 
\begin{itemize}
\item<3-> For $\tau=+\infty$, the merging condition $\min\{\tilde{f}(e),\tilde{f}(e_i)\}<\tilde{f}(i)+\tau$ trivially holds and merges occur according to the persistence theory.
\item<4-> The resulting PD can be used to choose $\tau$ for the second run.
\item<5-> Sort the peaks by prominence value and look for the largest drop. %weighed by height to avoid squeezing
\item<6-> Additional application-specific information can be used.
\end{itemize}
\end{itemize}
\end{frame}


\section{Theoretical guarantees}
\begin{frame}
\frametitle{Theoretical guarantees}
\begin{itemize}
\item<1-> Let $X$ be an $m$-dimensional Riemannian manifold with convexity radius $\rho>0$.
\item<2-> Let $f : X \to \R$ a $c$-Lipschitz-continuous probability density function with respect to the $m$-dimensional Hausdorff measure. 
\item<3-> Assume that the input data set $P$ has been sampled over $X$ according to $f$ in i.i.d. fashion, and that the values of $f$ at the data points and the geodesic distances in $X$ between the data points are known upt to small additive error. 
\item<4-> Assume the input graph $G$ to be the $\delta$-Rips graph built over $P$ using the estimated geodesic distances for some $\delta$.
%Recall that the convexity radius of X is the infimum over the points x in X of the supremum over the values
%r >= 0 such that any geodesic ball of center x and radius r0 < r is geodesically convex, that is, any two points
%in that ball are joined by a unique geodesic of length less than 2r0, and this geodesic is contained in the ball.
\end{itemize}
\end{frame}

\begin{frame}
\begin{definition}
Given two values $d_2 > d_1 \geq 0$, the persistence diagram $D_0f$ is called $(d_1, d_2)$-separated if every point of $D_0f$ lies either in the region $D_1$ above the diagonal line $y = x - d_1$, or in the region $D_2$ below the diagonal line $y = x - d_2$ and to the right
of the vertical line $x = d_2$.
\end{definition}
\begin{figure}
\centering
\includegraphics[scale=0.6]{separated}
\end{figure}
\end{frame}

\begin{frame}
\begin{theorem}
If $D_0f$ is $(d_1, d_2)$-separated and if $\delta<\min\{\rho, \frac{d_2-d_1}{5c}\}$, then for any $\tau\in (d_1 + 2c\delta, d_2 - 3c\delta)$ the number of clusters output by the algorithm is equal to the number of peaks of $f$ of prominence at least $\tau$ with probability at least $1 - e^{-\Omega(n)}$. %big Omega I think it is the one from number theory https://en.wikipedia.org/wiki/Big_O_notation#Family_of_Bachmann%E2%80%93Landau_notations
\end{theorem}\pause
The big-$\Omega$ notation hides a factor increasing monotonically with $c$ and $\delta$.
\begin{itemize}
\item<3-> The larger the prominence gap $d_2- d_1$, the
larger the range of admissible values for $\tau$.
\item<4-> The smaller $\delta$, the larger the range, but also the smaller
the probability of success. %makes sense because small delta may cause more peaks
\end{itemize}
\end{frame}

\begin{frame}
\begin{definition}
A \textbf{multi-bijection} between persistence diagrams $DX$ and $DY$ is a bijection
\[
\gamma:\bigcup_{p\in|DX|}\coprod_{i=1}^{\mu(p)}p\to \bigcup_{q\in|DY|}\coprod_{i=1}^{\mu(q)}q
\]
where $|DX|$ denotes the support of $DX$, i.e. the set $DX$ without any multiplicities, and where $\mu(p)$ denotes the multiplicity of point. 
%the support is thoug to be a subset of R^2 (including infinities)
\end{definition}\pause 
Note that such bijections always exist since the points on the diagonal have
infinite multiplicities. %so you can send the noise to the diagonal with no problem, and this doesn't measure the similarity, there is the bottleneck distance for that
\end{frame}

\begin{frame}
\begin{definition}
Given a subset $Y \subset X$ and $\varepsilon > 0$, $P$ is a \textbf{geodesic $\varepsilon$-
sample} of $Y$ if every point of $Y$ lies within geodesic distance $\varepsilon$ of $P$.
\end{definition}
\begin{itemize}
\item<2> Let $Q_\alpha^{NE}= (\alpha,+\infty]\times (\alpha,+\infty]$, $Q_\alpha^{SE}=(\alpha,+\infty]\times[-\infty,\alpha]$, $Q_\alpha^{SW}=[-\infty,\alpha]\times[-\infty,\alpha]$, and $Q_\alpha^{NE}=[-\infty,\alpha]\times(\alpha,+\infty]$. %quadrants

\item<3-> Let $D_0R_\delta(P)$ be the persistence diagram obtained from the $\delta$-Rips graph obtained from $P$ as an approximation of $D_0f$.
\end{itemize}
\end{frame}

\begin{frame}
\begin{theorem}
For any positive $\delta < \rho$, for
any $\alpha\in \R$ such that $P$ is a geodesic $\frac{\delta}{4}$-sample of $F^\alpha=f^{-1}((\alpha,+\infty])$, there is a multibijection $\gamma D_0f \to D_0R_\delta(P)$ such that:
\begin{itemize}
\item $\forall p\in D_0f\cap Q_\alpha^{NE}, ||p-\gamma(p)||_{\infty}\leq c\delta$.
\item $\forall q\in D_0R_\delta(P)\cap Q_\alpha^{NE}, ||\gamma^{-1}(q)-q||_{\infty}\leq c\delta$.
\item $\forall p\in D_0f\cap Q_\alpha^{SE}, |p_x-\gamma(p)_x|\leq c\delta$.
\item $\forall q\in D_0R_\delta(P)\cap Q_\alpha^{SE}, |\gamma^{-1}(q)_x-q_x|\leq c\delta$.
\end{itemize}
%the reason is that outside the superlevelset we don't have control, and there is a quadrant where there are no points so it is not included
\end{theorem}
\end{frame}

\begin{frame}
\begin{figure}
\centering
\includegraphics[scale=0.8]{multibijection}
\caption{Multibijection}
\end{figure}
\end{frame}
\begin{frame}
%Under the same hypotheses as previous theorem,
\begin{theorem}
 It holds with probability at least $1 - e^{-\Omega(n)}$ that for every point $p \in D_2$ the algorithm outputs
a cluster $C$ such that \[C \cap F^\alpha = B_\tau (m_p) \cap P \cap F^\alpha\] for all $\alpha \in(\alpha_\tau (m_p) + d_1 +
\frac{5}{2}c\delta, f(m_p)]$, where $m_p$ is the root of $p$, where $B_\tau (m_p)$ denotes 
the basin of attraction of $m_p$ in the underlying manifold $X$, and where $\alpha_\tau(m_p)$ is the first value of $\alpha$ at which $B_\tau(m_p)$ gets connected to the basin of attraction of another peak of
$f$ of prominence at least $\tau$ in the superlevel-set $F^\alpha$.
%m_p is the peak of f corresponding to p
\end{theorem}\pause
%We can approximate a stable part of the basins of attraction, even in the smooth case they are generally unstable
As a consequence each basin of attraction is stable under small perturbations of the function $f$ between certain values of $f$. %the algorithm can be run multiple times with small perturbations of f
\end{frame}
%\begin{frame}
%THE NON-TECHNICAL SECTION, IF THERE'S TIME GO MORE TECHNICAL AND USE THE NON-TECHNICAL AS EXPLANATION (MULTI-BIJECTIONS, INTERLEAVING DIAGRAMS -NO NEED TO DEFINE THEM, IT'S ON OTHER TOPICS, BUT INCLUDE A SLIDE JUST IN CASE)
%THE 4 POINTS OF SECTION 7 (NEED TO INCLUDE ASSUMPITON ON F AND ON THE MANIFOLD), THEOREM 7.2 (THIS IS RELATED TO THE QUARTERS) MAYBE MENTION THE TEXT BEFORE 7.3t
%EXPLAIN THE DIFFERENT QUARTERS ACCORDING TO ALPHA
%USE DEF 9.1 FOR SEPARATED WITH 16 AND THM 9.2
%THM 10.1 WITH EXPLANATION BELOW
%\end{frame}
\section{Experimental results}
\begin{frame}
\frametitle{Experimental results}
\begin{figure}
\centering
\includegraphics[scale=1]{rings}
\caption{Ring shaped clusters can also be detected.}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Image segmentation}
Images must be mapped to a color space. %Luv space
\begin{figure}
\centering
\includegraphics[scale=0.5]{segmentation}
\end{figure}
%tau can be chosen after checking different clustering results
\end{frame}

\begin{frame}
\begin{center}
\Huge{Thank you very much!}
\end{center}
\end{frame}


\end{document}
